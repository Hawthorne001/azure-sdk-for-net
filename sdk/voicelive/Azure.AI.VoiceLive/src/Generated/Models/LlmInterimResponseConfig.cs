// Copyright (c) Microsoft Corporation. All rights reserved.
// Licensed under the MIT License.

// <auto-generated/>

#nullable disable

using System;
using System.Collections.Generic;

namespace Azure.AI.VoiceLive
{
    /// <summary>
    /// Configuration for LLM-based interim response generation.
    /// Uses LLM to generate context-aware interim responses when any trigger condition is met.
    /// </summary>
    public partial class LlmInterimResponseConfig : InterimResponseConfigBase
    {
        /// <summary> Initializes a new instance of <see cref="LlmInterimResponseConfig"/>. </summary>
        public LlmInterimResponseConfig() : base(InterimResponseConfigType.LlmInterimResponse)
        {
        }

        /// <summary> Initializes a new instance of <see cref="LlmInterimResponseConfig"/>. </summary>
        /// <param name="type"> The type of interim response configuration. </param>
        /// <param name="triggers">
        /// List of triggers that can fire the interim response. Any trigger can activate it (OR logic).
        /// Supported: 'latency', 'tool'.
        /// </param>
        /// <param name="latencyThresholdMs"> Latency threshold in milliseconds before triggering interim response. Default is 2000ms. </param>
        /// <param name="additionalBinaryDataProperties"> Keeps track of any properties unknown to the library. </param>
        /// <param name="model"> The model to use for LLM-based interim response generation. Default is gpt-4.1-mini. </param>
        /// <param name="instructions"> Custom instructions for generating interim responses. If not provided, a default prompt is used. </param>
        /// <param name="maxCompletionTokens"> Maximum number of tokens to generate for the interim response. </param>
        internal LlmInterimResponseConfig(InterimResponseConfigType @type, IList<InterimResponseTrigger> triggers, int? latencyThresholdMs, IDictionary<string, BinaryData> additionalBinaryDataProperties, string model, string instructions, int? maxCompletionTokens) : base(@type, triggers, latencyThresholdMs, additionalBinaryDataProperties)
        {
            Model = model;
            Instructions = instructions;
            MaxCompletionTokens = maxCompletionTokens;
        }

        /// <summary> The model to use for LLM-based interim response generation. Default is gpt-4.1-mini. </summary>
        public string Model { get; set; }

        /// <summary> Custom instructions for generating interim responses. If not provided, a default prompt is used. </summary>
        public string Instructions { get; set; }

        /// <summary> Maximum number of tokens to generate for the interim response. </summary>
        public int? MaxCompletionTokens { get; set; }
    }
}
